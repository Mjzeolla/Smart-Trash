# -*- coding: utf-8 -*-
"""Main Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wAeSyVZkN-9Gnv19r2_z65XuqwkI5QID
"""

# !rm -rf /content/1690041674

"""The below cell handles importing all of the useful packages used during the project"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
import glob
import shutil
import PIL
import tensorflow as tf
import PIL.Image
import zipfile

# from google.colab import drive
# drive.mount('/content/drive')

"""Next the team uses the unzip package to unzip the data and place it into the respctive folders"""

SEED=42
!unzip '/content/trash_net' -d '/content'
!unzip '/content/yolo_labels' -d '/content'

def plot_images(images_arr):
    fig, axes = plt.subplots(1, len(images_arr), figsize=(20,20))
    axes = axes.flatten()
    for img, ax in zip( images_arr, axes):
        ax.imshow(img)
    plt.tight_layout()
    plt.show()

base_dir = '/content/trash_net/'
classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']
data_dir = '/content/trash_net_data'

"""Once the data is imported the splitfolders funtion is used to generate a 70/20/10 data split."""

!pip install split-folders
import splitfolders as sf
sf.ratio(base_dir, # The location of dataset
        output='trash_net_data', # The output location
        seed=SEED, # The number of seed
        ratio=(.7, .2, .1), # The ratio of splited dataset
        group_prefix=None, # If your dataset contains more than one file like ".jpg", ".pdf", etc
        move=False # If you choose to move, turn this into True
      )

"""For the YOLO models a seperate dataset is generated according to the required labels. The images are mapped to the labels as defined in the yolo_labels data."""

#Generates the YOLO dataset (since it needs labels)
yolo_dir = '/content/yolo_trash_net_data/'
label_dir = '/content/yolo_labels'

labels = glob.glob(label_dir + '/*.txt')
print("Labels: " + str(len(labels)))


for folder in ['test', 'train', 'val']:
  if not os.path.exists(os.path.join(yolo_dir, folder)):
      os.makedirs(os.path.join(yolo_dir, folder))
      os.makedirs(os.path.join(yolo_dir, folder, 'images'))
      os.makedirs(os.path.join(yolo_dir, folder, 'labels'))
  for cl in classes:
    img_path = os.path.join(data_dir,folder, cl)
    images = glob.glob(img_path + '/*.jpg')
    print("{} {}: {} Images".format(folder, cl, len(images)))
    for img in images:
      img_string = os.path.basename(img).replace('.jpg', '_jpg')
      label = list(filter(lambda x: img_string in x, labels))
      if len(label) == 1:
        label_name = os.path.basename(label[0])
        shutil.copy(img, os.path.join(yolo_dir, folder, 'images'))
        shutil.copy(label[0], os.path.join(yolo_dir, folder, 'labels'))
        dest_dir = os.path.join(yolo_dir, folder, 'labels')
        new_file = os.path.join(dest_dir, img_string.replace('_jpg', '.txt'))
        old_file = os.path.join(dest_dir, label_name)
        os.rename(old_file, new_file)

for folder in ['test', 'train', 'val']:
    img_path = os.path.join(yolo_dir, folder, 'images')
    images = glob.glob(img_path + '/*.jpg')
    label_path = os.path.join(yolo_dir, folder, 'labels')
    labels = glob.glob(label_path + '/*.txt')
    print("{}: {} Images".format(folder, len(images)))
    print("{}: {} Labels".format(folder, len(labels)))

# import shutil

#hutil.rmtree('/content/trash_net')
#shutil.rmtree('/content/trash_net_data')
#shutil.rmtree('/content/yolo_trash_net_data')
#shutil.rmtree('/content/yolo_labels')

"""Next, ImageAugmentation is performed using keras common functions.
The data is loaded into a Data Generator, and then augmented.
For example, training data is flipped, rotated, etc.

The validation and testing data remained the same.

THe team also prints some of the sample images after augmenation.
- Row 1 is the new training data exmaples
- Row 2 is the untouched testing data
"""

from keras.preprocessing.image import ImageDataGenerator
batch_size = 100

IMG_SHAPE = 224

train_dir = os.path.join(data_dir, 'train')
val_dir = os.path.join(data_dir, 'val')
test_dir = os.path.join(data_dir, 'test')


train_image_ds = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    rotation_range=45,
    width_shift_range=.15,
    height_shift_range=.15,
    horizontal_flip=True,
    vertical_flip=True,
    zoom_range=0.5
)

train_data_gen = train_image_ds.flow_from_directory(
  batch_size=batch_size,
  directory=train_dir,
  shuffle=True,
  seed=SEED,
  target_size=(IMG_SHAPE,IMG_SHAPE),
  class_mode='sparse',
)

val_image_ds = ImageDataGenerator(rescale=1./255)
test_image_ds = ImageDataGenerator(rescale=1./255)

validation_data_gen = val_image_ds.flow_from_directory(
  batch_size=batch_size,
  directory=val_dir,
  target_size=(IMG_SHAPE,IMG_SHAPE),
  class_mode='sparse',
  seed=SEED,
  shuffle=False,
)

test_data_gen = test_image_ds.flow_from_directory(
  batch_size=batch_size,
  directory=test_dir,
  target_size=(IMG_SHAPE,IMG_SHAPE),
  class_mode='sparse',
  seed=SEED,
  shuffle = False
)


test_labels = []

augmented_images = [train_data_gen[0][0][0] for i in range(5)]
plot_images(augmented_images)

augmented_images = [validation_data_gen[0][0][0] for i in range(5)]
plot_images(augmented_images)

x,y = validation_data_gen.next()
print(x.shape)
print(train_data_gen.num_classes)

EPOCHS = 100

from keras.utils.sidecar_evaluator import optimizer
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense,Conv2D, MaxPooling2D,Flatten, Dropout, Flatten
from tensorflow.keras.models import Model, Sequential

"""The team then creates some functions for getting the performace metrics.
SKLearn functions allowed the team to calculate these items easily.

multiclass_roc_auc_score:
  - Function to calcualte the roc curve and scores

plot_training_history
  - Function to plot the training history for each epoch by accuracy and loss

analyze_predictions
  - Wrapper function for calcualting the performance metrics

-the Maco average was implemented b/c it works best on imbalanced datasets
"""

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import roc_curve, auc
from itertools import cycle
from sklearn import preprocessing
from sklearn.metrics import roc_auc_score

def multiclass_roc_auc_score(y_test, y_pred, average="macro"):
    fig, c_ax = plt.subplots(1,1, figsize = (12, 8))

    lb = preprocessing.LabelBinarizer()
    lb.fit(y_true)
    y_test = lb.transform(y_test)
    y_pred = lb.transform(y_pred)

    for (idx, c_label) in enumerate(classes):
        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])
        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))
    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')

    c_ax.legend()
    c_ax.set_xlabel('False Positive Rate')
    c_ax.set_ylabel('True Positive Rate')
    v = roc_auc_score(y_test, y_pred, average=average)
    print('ROC AUC score:',v)
    plt.show()
    return v

def plot_training_history(history, epochs, plot_metric="accuracy", plot_metric_label="Accuracy"):
  acc = history.history[plot_metric]
  val_acc = history.history['val_'+plot_metric]

  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs_range = range(epochs)

  plt.figure(figsize=(8, 8))
  plt.subplot(1, 2, 1)
  plt.plot(epochs_range, acc, label='Training '+plot_metric_label)
  plt.plot(epochs_range, val_acc, label='Validation '+plot_metric_label)
  plt.legend(loc='lower right')
  plt.title('Training and Validation '+plot_metric_label)
  plt.xlabel('EPOCHS')

  plt.subplot(1, 2, 2)
  plt.plot(epochs_range, loss, label='Training Loss')
  plt.plot(epochs_range, val_loss, label='Validation Loss')
  plt.legend(loc='upper right')
  plt.xlabel('EPOCHS')
  plt.title('Training and Validation Loss')
  plt.show()

def analyze_predictions(preds, true_vals, avg='macro'):
  print("Precision Score: ", precision_score(true_vals, preds, average=avg))
  print("Recall Score: ",recall_score(true_vals, preds, average=avg))
  print("F1 Score: ",f1_score(true_vals, preds, average=avg))
  print("Accuracy Score: ",accuracy_score(true_vals, preds))
  print('MCC Score: ', matthews_corrcoef(true_vals, preds))
  multiclass_roc_auc_score(true_vals, preds)

"""The team also generated a custom model for importing items from TensorFlow Hub
Sets the base layers as non-trainable, and sets the basic 5 class output layer.
"""

import tensorflow_hub as hub
IMAGE_RES = 224 #Model trained on 224 x 224 images
num_classes = len(classes)
EPOCHS = 10

def importModel(URL, activation=None):

  feature_extractor = hub.KerasLayer(URL,
    input_shape=(IMAGE_RES, IMAGE_RES, 3),
    trainable=False)

  return tf.keras.Sequential([
    feature_extractor,
     tf.keras.layers.Dense(6, activation) if(activation) else tf.keras.layers.Dense(6),
])

RESNET_50_URL = "https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5"
RESNET_101_URL = "https://tfhub.dev/google/imagenet/resnet_v2_101/feature_vector/5"
RESNET_152_URL = "https://tfhub.dev/google/imagenet/resnet_v2_152/classification/5"


model_custom_resnet_50 = importModel(RESNET_50_URL)
model_custom_resnet_101 = importModel(RESNET_101_URL)
model_custom_resnet_152 = importModel(RESNET_152_URL)

model_custom_resnet_50.summary()

model_custom_resnet_101.summary()

model_custom_resnet_152.summary()

"""The code below compiles the basic ResNet models  (50, 101, and 152)
- Optiizer is ADAM
- Loss is SparseCategoricalCrossentropy
- Metrics to track is Accuracy
"""

model_custom_resnet_50.compile(
  optimizer='adam',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

model_custom_resnet_101.compile(
  optimizer='adam',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

model_custom_resnet_152.compile(
  optimizer='adam',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

"""The following code blocks calcualtes the training for each ResNet model"""

history_resnet_50 = model_custom_resnet_50.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

"""Prints the training hsitory for ResNet-50"""

plot_training_history(history_resnet_50, EPOCHS)

EPOCHS = 70
history_resnet_101 = model_custom_resnet_101.fit(train_data_gen,
                      epochs=EPOCHS,
                      validation_data=validation_data_gen)

"""Prints the training hsitory for ResNet-101"""

plot_training_history(history_resnet_101, EPOCHS)

EPOCHS = 70
history_resnet_152 = model_custom_resnet_152.fit(train_data_gen,
                      epochs=EPOCHS,
                      validation_data=validation_data_gen)

"""Prints the training hsitory for ResNet-152"""

plot_training_history(history_resnet_152, EPOCHS)

"""Print the performance metrics for ResNet-50
- Perforance is calcualted by trying to predict never before seen testing data
"""

y_true = test_data_gen.classes
predictions = model_custom_resnet_50.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_custom_resnet_50.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

"""Print the performance metrics for ResNet-101
- Perforance is calcualted by trying to predict never before seen testing data
"""

y_true = test_data_gen.classes
predictions = model_custom_resnet_101.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_custom_resnet_101.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

"""Print the performance metrics for ResNet-152
- Perforance is calcualted by trying to predict never before seen testing data
"""

y_true = test_data_gen.classes
predictions = model_custom_resnet_152.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_custom_resnet_152.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

"""This code block imports the VGG-16 models form the tensorflow package.
- It sets the layers as non-trainable
- Uses multiple hidden layers to effectively tranform lay outputs
"""

VGG16 = tf.keras.applications.vgg16.VGG16(
    include_top=False,
    weights='imagenet',
    input_shape=(IMG_SHAPE, IMG_SHAPE, 3)
)

VGG16.trainable=False
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
prediction_layer = tf.keras.layers.Dense(len(classes))

top_model = VGG16.output
top_model = Flatten(name="flatten")(top_model)
top_model = Dense(4096, activation='relu')(top_model)
top_model = Dense(1072, activation='relu')(top_model)
output_layer = Dense(len(classes))(top_model)


model_vgg_16 = Model(inputs=VGG16.input, outputs=output_layer)

"""This code block imports the VGG-19 models form the tensorflow package.
- It sets the layers as non-trainable
- Uses multiple hidden layers to effectively tranform lay outputs
"""

VGG19 = tf.keras.applications.vgg19.VGG19(
    include_top=False,
    weights='imagenet',
    input_shape=(IMG_SHAPE, IMG_SHAPE, 3)
)

VGG19.trainable=False
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
prediction_layer = tf.keras.layers.Dense(len(classes))

top_model = VGG19.output
top_model = Flatten(name="flatten")(top_model)
top_model = Dense(4096, activation='relu')(top_model)
top_model = Dense(1072, activation='relu')(top_model)
output_layer = Dense(len(classes))(top_model)


model_vgg_19 = Model(inputs=VGG19.input, outputs=output_layer)

"""The next two code blocks compile the imported VGG models
- Optimizer is ADAM
- Loss is SparseCategoricalCrossentropy
- Metric is Accuracy and Loss
"""

model_vgg_16.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

model_vgg_19.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

"""This code block trains the VGG-16 model over 70 Epochs"""

EPOCHS=70
history_vgg_16 = model_vgg_16.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

"""This code block trains the VGG-19 model over 70 Epochs"""

EPOCHS=70
history_vgg_19 = model_vgg_19.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

"""This code block prints the training history plot for VGG-16 for loss and accuracy."""

plot_training_history(history_vgg_16, EPOCHS)

"""This code block prints the training history plot for VGG-19 for loss and accuracy"""

plot_training_history(history_vgg_19, EPOCHS)

"""Using the trained VGG-16 model the team predicted the outputs for new testing data. And then calcualted performance metrics on the predicted labels."""

y_true = test_data_gen.classes
predictions = model_vgg_16.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_vgg_16.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

"""Using the trained VGG-19 model the team predicted the outputs for new testing data. And then calcualted performance metrics on the predicted labels."""

y_true = test_data_gen.classes
predictions = model_vgg_19.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_vgg_19.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

"""saveModel:
- Function to save a model as a .pb model
- This function was used to save the trained models which can then be inported later.
"""

import time
from tensorflow import saved_model
from google.colab import files

def saveModel(model, export_path=None, download=False):
  t = time.time()

  export_path_sm = export_path if export_path else "./{}".format(int(t))
  print(export_path_sm)

  saved_model.save(model, export_path_sm)

  if download:
    files.download(export_path_sm + '/saved_model.pb')

saveModel(model_vgg_19, download=True)

saveModel(model_custom_resnet_152, download=True)

!pip install tensorflow_addons

"""This code block re-imports and compiles the ResNet-50 model
- Optimizer: SGD
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss
"""

#AUC Metric Resnet 50
import tensorflow_addons as tfa
import tensorflow as tf

model_custom_resnet_50_sgd = importModel(RESNET_50_URL)


model_custom_resnet_50_sgd.compile(
  optimizer='sgd',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

"""The following code blocks handle, training, testing and plotting the ResNet-50 SGD data results."""

EPOCHS = 50
history_resnet_50_sgd = model_custom_resnet_50_sgd.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

plot_training_history(history_resnet_50_sgd, EPOCHS)

y_true = test_data_gen.classes
predictions = model_custom_resnet_50_sgd.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_custom_resnet_50_sgd.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

saveModel(model_custom_resnet_50_sgd, download=True)

"""This code block re-imports and compiles the ResNet-101 model
- Optimizer: SGD
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss
"""

model_custom_resnet_101_sgd = importModel(RESNET_101_URL)


model_custom_resnet_101_sgd.compile(
  optimizer='sgd',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

"""The following code blocks handle, training, testing and plotting the ResNet-101 SGD data results."""

EPOCHS = 50
history_resnet_101_sgd = model_custom_resnet_101_sgd.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

plot_training_history(history_resnet_101_sgd, EPOCHS)

y_true = test_data_gen.classes
predictions = model_custom_resnet_101_sgd.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_custom_resnet_101_sgd.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

saveModel(model_custom_resnet_101_sgd, download=True)

"""This code block re-imports and compiles the ResNet-152 model
- Optimizer: SGD
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss
"""

model_custom_resnet_152_sgd = importModel(RESNET_152_URL)


model_custom_resnet_152_sgd.compile(
  optimizer='sgd',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

"""The following code blocks handle, training, testing and plotting the ResNet-152 SGD data results."""

EPOCHS = 40
history_resnet_152_sgd = model_custom_resnet_152_sgd.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

plot_training_history(history_resnet_152_sgd, EPOCHS)

y_true = test_data_gen.classes
predictions = model_custom_resnet_152_sgd.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_custom_resnet_152_sgd.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

saveModel(model_custom_resnet_152_sgd, download=True)

"""This code block re-imports and compiles the ResNet-50 model
- Optimizer: AdamW
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss
"""

model_custom_resnet_50_adamW = importModel(RESNET_50_URL)

model_custom_resnet_50_adamW.compile(
  optimizer=tf.keras.optimizers.AdamW(),
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

"""The following code blocks handle, training, testing and plotting the ResNet-50 AdamW data results."""

EPOCHS = 60
history_resnet_50_adamW = model_custom_resnet_50_adamW.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

plot_training_history(history_resnet_50_adamW, EPOCHS)

y_true = test_data_gen.classes
predictions = model_custom_resnet_50_adamW.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_custom_resnet_50_adamW.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

saveModel(model_custom_resnet_152_sgd, download=True)

"""This code block re-imports and compiles the ResNet-101 model
- Optimizer: AdamW
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss
"""

model_custom_resnet_101_adamW = importModel(RESNET_101_URL)

model_custom_resnet_101_adamW.compile(
  optimizer=tf.keras.optimizers.AdamW(),
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

"""The following code blocks handle, training, testing and plotting the ResNet-50 AdamW data results."""

EPOCHS = 60
history_resnet_101_adamW = model_custom_resnet_101_adamW.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

plot_training_history(history_resnet_101_adamW, EPOCHS)

y_true = test_data_gen.classes
predictions = model_custom_resnet_101_adamW.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_custom_resnet_101_adamW.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

saveModel(model_custom_resnet_101_adamW, download=True)

"""This code block re-imports and compiles the ResNet-152 model
- Optimizer: AdamW
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss
"""

model_custom_resnet_152_adamW = importModel(RESNET_152_URL)

model_custom_resnet_152_adamW.compile(
  optimizer=tf.keras.optimizers.AdamW(),
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

"""The following code blocks handle, training, testing and plotting the ResNet-50 AdamW data results."""

EPOCHS = 60
history_resnet_152_adamW = model_custom_resnet_152_adamW.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

plot_training_history(history_resnet_152_adamW, EPOCHS)

y_true = test_data_gen.classes
predictions = model_custom_resnet_152_adamW.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_custom_resnet_152_adamW.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

saveModel(model_custom_resnet_152_adamW, download=True)

"""This code block re-imports and compiles the VGG-16 model
- Optimizer: SGD
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss
- THe model aritecture remained the same
"""

VGG16_SGD = tf.keras.applications.vgg16.VGG16(
    include_top=False,
    weights='imagenet',
    input_shape=(IMG_SHAPE, IMG_SHAPE, 3)
)

VGG16_SGD.trainable=False
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
prediction_layer = tf.keras.layers.Dense(len(classes))

top_model = VGG16_SGD.output
top_model = Flatten(name="flatten")(top_model)
top_model = Dense(4096, activation='relu')(top_model)
top_model = Dense(1072, activation='relu')(top_model)
output_layer = Dense(len(classes))(top_model)


model_vgg_16_SGD = Model(inputs=VGG16_SGD.input, outputs=output_layer)

model_vgg_16_SGD.compile(optimizer='sgd',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

"""The following code blocks handle, training, testing and plotting the VGG-16 SGD data results."""

EPOCHS=70
history_vgg_16_SGD = model_vgg_16_SGD.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

plot_training_history(history_vgg_16_SGD, EPOCHS)

y_true = test_data_gen.classes
predictions = model_vgg_16_SGD.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_vgg_16_SGD.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

saveModel(model_vgg_16_SGD, download=True)

"""This code block re-imports and compiles the VGG-19 model
- Optimizer: SGD
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss
- THe model aritecture remained the same
"""

VGG19_SGD = tf.keras.applications.vgg19.VGG19(
    include_top=False,
    weights='imagenet',
    input_shape=(IMG_SHAPE, IMG_SHAPE, 3)
)

VGG19_SGD.trainable=False
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
prediction_layer = tf.keras.layers.Dense(len(classes))

top_model = VGG19_SGD.output
top_model = Flatten(name="flatten")(top_model)
top_model = Dense(4096, activation='relu')(top_model)
top_model = Dense(1072, activation='relu')(top_model)
output_layer = Dense(len(classes))(top_model)


model_vgg_19_SGD = Model(inputs=VGG19_SGD.input, outputs=output_layer)

model_vgg_19_SGD.compile(optimizer='sgd',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

"""The following code blocks handle, training, testing and plotting the VGG-19 SGD data results."""

EPOCHS=70
history_vgg_19_SGD = model_vgg_19_SGD.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

plot_training_history(history_vgg_19_SGD, EPOCHS)

y_true = test_data_gen.classes
predictions = model_vgg_19_SGD.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_vgg_19_SGD.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

saveModel(model_vgg_19_SGD, download=True)

"""This code block re-imports and compiles the VGG-16 model
- Optimizer: AdamW
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss
- THe model aritecture remained the same
"""

VGG16_AdamW = tf.keras.applications.vgg16.VGG16(
    include_top=False,
    weights='imagenet',
    input_shape=(IMG_SHAPE, IMG_SHAPE, 3)
)

VGG16_AdamW.trainable=False
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
prediction_layer = tf.keras.layers.Dense(len(classes))

top_model = VGG16_AdamW.output
top_model = Flatten(name="flatten")(top_model)
top_model = Dense(4096, activation='relu')(top_model)
top_model = Dense(1072, activation='relu')(top_model)
output_layer = Dense(len(classes))(top_model)


model_vgg_16_AdamW = Model(inputs=VGG16_AdamW.input, outputs=output_layer)

model_vgg_16_AdamW.compile(optimizer=tf.keras.optimizers.AdamW(),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

"""The following code blocks handle, training, testing and plotting the VGG-16 AdamW data results."""

EPOCHS=70
history_vgg_16_AdamW = model_vgg_16_AdamW.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

plot_training_history(history_vgg_16_AdamW, EPOCHS)

y_true = test_data_gen.classes
predictions = model_vgg_16_AdamW.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_vgg_16_AdamW.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

saveModel(model_vgg_16_AdamW, download=True)

"""This code block re-imports and compiles the VGG-19 model
- Optimizer: AdamW
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss
- THe model aritecture remained the same
"""

VGG19_AdamW = tf.keras.applications.vgg19.VGG19(
    include_top=False,
    weights='imagenet',
    input_shape=(IMG_SHAPE, IMG_SHAPE, 3)
)

VGG19_AdamW.trainable=False
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
prediction_layer = tf.keras.layers.Dense(len(classes))

top_model = VGG19_AdamW.output
top_model = Flatten(name="flatten")(top_model)
top_model = Dense(4096, activation='relu')(top_model)
top_model = Dense(1072, activation='relu')(top_model)
output_layer = Dense(len(classes))(top_model)


model_vgg_19_AdamW = Model(inputs=VGG19_AdamW.input, outputs=output_layer)

model_vgg_19_AdamW.compile(optimizer=tf.keras.optimizers.AdamW(),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

"""The following code blocks handle, training, testing and plotting the VGG-19 AdamW data results."""

EPOCHS=70
history_vgg_19_AdamW = model_vgg_19_AdamW.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

plot_training_history(history_vgg_19_AdamW, EPOCHS)

y_true = test_data_gen.classes
predictions = model_vgg_19_AdamW.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_vgg_19_AdamW.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

saveModel(model_vgg_19_AdamW, download=True)

"""This code block re-imports and compiles the ResNet-50 model

- Optimizer: Adam
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss
"""

resnet_50_learning_rate_optimzer = importModel(RESNET_50_URL)

resnet_50_learning_rate_optimzer.compile(optimizer=tf.keras.optimizers.Adam(),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

"""The code fits the training data to the new mode, BUT adds the LearningRateScheduler. The LearningRateScheduler will reduce the learning rate overtime."""

EPOCHS = 70
history_resnet_50_learning_rate_optimzer = resnet_50_learning_rate_optimzer.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen,
                    callbacks=[
                        tf.keras.callbacks.LearningRateScheduler(
                            lambda epoch: 1e-3 * 10 ** (epoch / 30)
                        )
                    ]
)

saveModel(resnet_50_learning_rate_optimzer, download=True)

"""Prints the Learning Rate vs Epoch plot"""

from matplotlib import rcParams

rcParams['figure.figsize'] = (18,8)
rcParams['axes.spines.top'] = False
rcParams['axes.spines.right'] = False


plt.plot(
    np.arange(1,71),
    history_resnet_50_learning_rate_optimzer.history['lr'],
    label='Learning Rate', lw=3, linestyle='--'
)

plt.xlabel('Epoch', size=14)
plt.ylabel('Learning Rate', size=14)
plt.title('Learning Rate vs Epoch', size=20)

"""Prints the Learning Rate vs Loss plot"""

EPOCHS = 70
learning_rates = 1e-3 * (10 ** (np.arange(EPOCHS) / 30))
plt.semilogx(learning_rates, history_resnet_50_learning_rate_optimzer.history['val_loss'], lw=3, color="#000")
plt.title('Learning Rate vs Loss', size = 20)
plt.xlabel('Learning Rate', size = 14)
plt.ylabel('Loss', size = 14)

min_loss = history_resnet_50_learning_rate_optimzer.history['val_loss'].index(min(history_resnet_50_learning_rate_optimzer.history['val_loss']))
print(learning_rates[min_loss])

"""Prints the Learning Rate vs validation accuracy plot"""

plt.semilogx(learning_rates, history_resnet_50_learning_rate_optimzer.history['val_accuracy'], lw=3, color="#000")
plt.title('Learning Rate vs Accuracy', size = 20)
plt.xlabel('Learning Rate', size = 14)
plt.ylabel('Accuracy', size = 14)

"""This code block re-imports and compiles the ResNet-50 model BYT uses the ReduceLROnPlateau. ReduceLROnPlateau will reduce the learning rate of the model whenever it beings to plateau in performance

- Optimizer: Adam
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss

ReduceLROnPlateau:
- monitor='val_loss' -- Metric to check
- factor=0.5 -- factor to reduce LR by each time
- patience = 5 -- Number of epochs to wait before reducing LR
"""

from tensorflow.keras.callbacks import ReduceLROnPlateau
EPOCHS = 70
# Create the ReduceLROnPlateau callback
reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
learning_rate = 1
resnet_50_learning_rate_optimzer_2 = importModel(RESNET_50_URL)

resnet_50_learning_rate_optimzer_2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)

"""The following code trains the new ResNet-50 ReduceLROnPlateau model.
It also shows whenever the learnign rate was reduced in the training logs.
"""

history_resnet_50_learning_rate_optimzer_2 = resnet_50_learning_rate_optimzer_2.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen,
                   callbacks=[reduce_lr_callback])

saveModel(resnet_50_learning_rate_optimzer_2, download=True)

"""Plots learning rate vs epoch overtime during training"""

from matplotlib import rcParams

rcParams['figure.figsize'] = (18,8)
rcParams['axes.spines.top'] = False
rcParams['axes.spines.right'] = False


plt.plot(
    np.arange(1,EPOCHS + 1),
    history_resnet_50_learning_rate_optimzer_2.history['lr'],
    label='Learning Rate', lw=3, linestyle='--'
)

plt.xlabel('Epoch', size=14)
plt.ylabel('Learning Rate', size=14)
plt.title('Learning Rate vs Epoch', size=20)

"""Plots learning rate vs loss overtime during training"""

learning_rates = history_resnet_50_learning_rate_optimzer_2.history['lr']
plt.semilogx(learning_rates, history_resnet_50_learning_rate_optimzer_2.history['val_loss'], lw=3, color="#000")
plt.title('Learning Rate vs Loss', size = 20)
plt.xlabel('Learning Rate', size = 14)
plt.ylabel('Loss', size = 14)
plt.gca().invert_xaxis()

min_loss = history_resnet_50_learning_rate_optimzer_2.history['val_loss'].index(min(history_resnet_50_learning_rate_optimzer_2.history['val_loss']))
print(learning_rates[min_loss])

"""Plots learning rate vs validation accuracy overtime during training"""

plt.semilogx(learning_rates, history_resnet_50_learning_rate_optimzer_2.history['val_accuracy'], lw=3, color="#000")
plt.title('Learning Rate vs Accuracy', size = 20)
plt.xlabel('Learning Rate', size = 14)
plt.ylabel('Accuracy', size = 14)
plt.gca().invert_xaxis()

"""Plots learning rate vs validation accuracy overtime during training"""

epochs = range(1, len(history_resnet_50_learning_rate_optimzer_2.history['val_accuracy']) + 1)
plt.plot(epochs, history_resnet_50_learning_rate_optimzer_2.history['val_accuracy'], 'bo-', label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy vs. Epoch')
plt.grid(True)
plt.legend(loc='lower right')

# Create a step plot for learning rates
plt.step(epochs, learning_rates, 'r-', where='post', label='Learning Rate')
plt.ylabel('Learning Rate')
plt.legend(loc='upper right')

"""Predicts the new model on the testing data and prints performance metrics and graphs."""

y_true = test_data_gen.classes
predictions = resnet_50_learning_rate_optimzer_2.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = resnet_50_learning_rate_optimzer_2.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

"""This code block re-imports and compiles the ResNet-50 model BUT uses the ReduceLROnPlateau. ReduceLROnPlateau will reduce the learning rate of the model whenever it beings to plateau in performance

- Optimizer: SGD
- Loss: SparseCategoricalCrossentropy
- Metrics: Accuracy and Loss

ReduceLROnPlateau:
- monitor='val_loss' -- Metric to check
- factor=0.5 -- factor to reduce LR by each time
- patience = 5 -- Number of epochs to wait before reducing LR
"""

from tensorflow.keras.callbacks import ReduceLROnPlateau
EPOCHS = 70
# Create the ReduceLROnPlateau callback
learning_rate = 1
resnet_50_SGD_ReduceLROnPlateau = importModel(RESNET_50_URL)

resnet_50_SGD_ReduceLROnPlateau.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)

"""Trains the model using the training date for the ReduceLROnPlateau model"""

EPOCHS = 70
history_resnet_50_SGD_ReduceLROnPlateau = resnet_50_SGD_ReduceLROnPlateau.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen,
                   callbacks=[reduce_lr_callback])

saveModel(resnet_50_SGD_ReduceLROnPlateau, download=True)

"""Plot the training metrics over each epoch"""

plot_training_history(history_resnet_50_SGD_ReduceLROnPlateau, EPOCHS)

"""Plot the Learning rate over each Epoch"""

from matplotlib import rcParams

rcParams['figure.figsize'] = (18,8)
rcParams['axes.spines.top'] = False
rcParams['axes.spines.right'] = False


plt.plot(
    np.arange(1,EPOCHS + 1),
    history_resnet_50_SGD_ReduceLROnPlateau.history['lr'],
    label='Learning Rate', lw=3, linestyle='--'
)

plt.xlabel('Epoch', size=14)
plt.ylabel('Learning Rate', size=14)
plt.title('Learning Rate vs Epoch', size=20)

"""Plot the Learning rate verses the validation loss"""

learning_rates = history_resnet_50_SGD_ReduceLROnPlateau.history['lr']
plt.semilogx(learning_rates, history_resnet_50_SGD_ReduceLROnPlateau.history['val_loss'], lw=3, color="#000")
plt.title('Learning Rate vs Loss', size = 20)
plt.xlabel('Learning Rate', size = 14)
plt.ylabel('Loss', size = 14)

min_loss = history_resnet_50_SGD_ReduceLROnPlateau.history['val_loss'].index(min(history_resnet_50_SGD_ReduceLROnPlateau.history['val_loss']))
print(learning_rates[min_loss])

"""Plot the Learning rate verses the validation accuract"""

plt.plot(np.arange(1,EPOCHS + 1), history_resnet_50_SGD_ReduceLROnPlateau.history['val_accuracy'], lw=3, color="#000")
plt.title('Learning Rate vs Validation  Accuracy', size = 20)
plt.xlabel('Learning Rate', size = 14)
plt.ylabel('Validation Accuracy', size = 14)

"""Plot the Learning rate verses the validation accuracy vs epoch"""

epochs = range(1, len(history_resnet_50_SGD_ReduceLROnPlateau.history['val_accuracy']) + 1)
plt.plot(epochs, history_resnet_50_SGD_ReduceLROnPlateau.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy vs. Epoch')

# Create a step plot for learning rates
plt.step(epochs, learning_rates, where='post', label='Learning Rate')
plt.legend(loc='upper right')

"""Predicts new inputs using the testing data and plots it"""

y_true = test_data_gen.classes
predictions = resnet_50_SGD_ReduceLROnPlateau.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = resnet_50_SGD_ReduceLROnPlateau.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

"""Generate the VGG-19 model for VGG19_Adam_ReduceLROnPlateau using the Adam Optimizer"""

VGG19_Adam_ReduceLROnPlateau = tf.keras.applications.vgg19.VGG19(
    include_top=False,
    weights='imagenet',
    input_shape=(IMG_SHAPE, IMG_SHAPE, 3)
)

VGG19_Adam_ReduceLROnPlateau.trainable=False
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
prediction_layer = tf.keras.layers.Dense(len(classes))

top_model = VGG19_Adam_ReduceLROnPlateau.output
top_model = Flatten(name="flatten")(top_model)
top_model = Dense(4096, activation='relu')(top_model)
top_model = Dense(1072, activation='relu')(top_model)
output_layer = Dense(len(classes))(top_model)


VGG16_Adam_ReduceLROnPlateau = Model(inputs=VGG19_Adam_ReduceLROnPlateau.input, outputs=output_layer)

VGG19_Adam_ReduceLROnPlateau.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

"""Train the model on the training data over 70 epochs"""

EPOCHS=70
from tensorflow.keras.callbacks import ReduceLROnPlateau
reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)

history_VGG19_Adam_ReduceLROnPlateau = VGG19_Adam_ReduceLROnPlateau.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen,
                    callbacks=[reduce_lr_callback])

saveModel(VGG16_Adam_ReduceLROnPlateau, download=True)

"""PLot training performance over each epoch"""

plot_training_history(history_VGG19_Adam_ReduceLROnPlateau, EPOCHS)

"""Plot learning rate vs the epoch"""

from matplotlib import rcParams

rcParams['figure.figsize'] = (18,8)
rcParams['axes.spines.top'] = False
rcParams['axes.spines.right'] = False


plt.plot(
    np.arange(1,EPOCHS + 1),
    history_VGG19_Adam_ReduceLROnPlateau.history['lr'],
    label='Learning Rate', lw=3, linestyle='--'
)

plt.xlabel('Epoch', size=14)
plt.ylabel('Learning Rate', size=14)
plt.title('Learning Rate vs Epoch', size=20)

"""Plot learning rate vs the validation loss"""

learning_rates = history_VGG19_Adam_ReduceLROnPlateau.history['lr']
plt.semilogx(learning_rates, history_VGG19_Adam_ReduceLROnPlateau.history['val_loss'], lw=3, color="#000")
plt.title('Learning Rate vs Loss', size = 20)
plt.xlabel('Learning Rate', size = 14)
plt.ylabel('Loss', size = 14)

min_loss = history_VGG19_Adam_ReduceLROnPlateau.history['val_loss'].index(min(history_VGG19_Adam_ReduceLROnPlateau.history['val_loss']))
print(learning_rates[min_loss])

"""Plot learning rate vs the validation accuracy"""

plt.plot(np.arange(1,EPOCHS + 1), history_VGG19_Adam_ReduceLROnPlateau.history['val_accuracy'], lw=3, color="#000")
plt.title('Learning Rate vs Validation  Accuracy', size = 20)
plt.xlabel('EPOCH', size = 14)
plt.ylabel('Validation Accuracy', size = 14)

"""Plot learning rate vs the validation accuracy vs epoch"""

epochs = range(1, len(history_VGG19_Adam_ReduceLROnPlateau.history['val_accuracy']) + 1)
plt.plot(epochs, history_VGG19_Adam_ReduceLROnPlateau.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy vs. Epoch')

# Create a step plot for learning rates
plt.step(epochs, learning_rates, where='post', label='Learning Rate')
plt.legend(loc='upper right')

"""Predict the data for the new VGG-19 Adam ReduceLROnPLateau and plot results"""

y_true = test_data_gen.classes
predictions = VGG16_Adam_ReduceLROnPlateau.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = VGG16_Adam_ReduceLROnPlateau.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

VGG19_custom = tf.keras.applications.vgg19.VGG19(
    include_top=False,
    weights='imagenet',
    input_shape=(IMG_SHAPE, IMG_SHAPE, 3)
)

VGG19_custom.trainable=False
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
prediction_layer = tf.keras.layers.Dense(len(classes))

top_model = VGG19_custom.output
top_model = Flatten(name="flatten")(top_model)
top_model = Dense(4096, activation='relu')(top_model)
top_model = Dropout(0.5)(top_model)
top_model = Dense(4096, activation='relu')(top_model)
top_model = Dense(4096 * 2, activation='relu')(top_model)
top_model = Dropout(0.5)(top_model)
top_model = Dense(1072, activation='relu')(top_model)
output_layer = Dense(len(classes))(top_model)





model_vgg_19_custom = Model(inputs=VGG19_custom.input, outputs=output_layer)

model_vgg_19_custom.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])

EPOCHS=70
history_vgg_19_custom = model_vgg_19_custom.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

plot_training_history(history_vgg_19_custom, EPOCHS)

y_true = test_data_gen.classes
predictions = model_vgg_19_custom.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = model_vgg_19_custom.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

"""The following code is for an in person demo.
- Compiles and trains new ResNet-50 model
- Predicts the evaluation data and plots it
- Plots on the predicted images and its classifier
"""

RESNET_50_URL = "https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5"

demo_resnet_50 = importModel(RESNET_50_URL)

demo_resnet_50.compile(
  optimizer='adam',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

EPOCHS=5
demo_history_resnet_50 = demo_resnet_50.fit(train_data_gen,
                    epochs=EPOCHS,
                    validation_data=validation_data_gen)

plot_training_history(demo_history_resnet_50, EPOCHS)

y_true = test_data_gen.classes
predictions = demo_resnet_50.predict(x=test_data_gen, steps=len(test_data_gen), verbose=0)

eval = demo_resnet_50.evaluate(x=test_data_gen, steps=len(test_data_gen), verbose=0)
print('Eval: ', eval)

y_preds = predictions.argmax(axis=1)

analyze_predictions(y_preds, y_true)

print(y_preds)
for i in range(1):
  print(classes[y_preds[i]])
  img = test_data_gen[0][0][0]
  plt.imshow(img)